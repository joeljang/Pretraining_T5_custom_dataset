adam_epsilon: 1.0e-08
early_stop_callback: false
eval_batch_size: 8
fp_16: false
freeze_embeds: false
freeze_encoder: false
gradient_accumulation_steps: 1
learning_rate: 0.001
max_grad_norm: 1.0
max_input_length: '128'
max_output_length: '128'
model_name_or_path: t5-base
n_gpu: 1
n_test: -1
n_train: -1
n_val: 0
num_train_epochs: 1
opt_level: O1
output_dir: t5_pretraining
resume_from_checkpoint: null
seed: 101
tokenizer_name_or_path: t5-base
train_batch_size: 8
val_check_interval: 1.0
val_percent_check: 0
warmup_steps: 0
weight_decay: 0.0
